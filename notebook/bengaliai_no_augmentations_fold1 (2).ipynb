{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2htaYO-fjk9R"
   },
   "outputs": [],
   "source": [
    "!pip install --user iterative-stratification --upgrade\n",
    "!pip install --user kaggle --upgrade\n",
    "!pip install --user pretrainedmodels --upgrade\n",
    "!pip install --user --upgrade efficientnet-pytorch\n",
    "!pip install --user albumentations --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6jVfA7zw2ej"
   },
   "outputs": [],
   "source": [
    "!sudo mkdir /home/kaushal28shah/.kaggle/\n",
    "!sudo mv kaggle.json /home/kaushal28shah/.kaggle/\n",
    "!sudo chmod 777 /home/kaushal28shah/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "fxabtCnoxOAa",
    "outputId": "82bac6d9-1246-4904-b80e-8df86de4b825"
   },
   "outputs": [],
   "source": [
    "!/home/kaushal28shah/.local/bin/kaggle datasets download -d bengaliai-image-pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "mL8c6yaLxQTe",
    "outputId": "c41f3905-495e-4990-ac5a-d9484a565001"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!sudo unzip -qq bengaliai-image-pickles.zip\n",
    "!sudo rm -rf bengaliai-image-pickles.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4z6yoETVkE1y",
    "outputId": "cd2611e0-3c10-47d4-b05e-8631202b967e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.augmentations import functional as F\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "def int_parameter(level, maxval):\n",
    "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
    "    Args:\n",
    "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
    "    maxval: Maximum value that the operation can have. This will be scaled to\n",
    "      level/PARAMETER_MAX.\n",
    "    Returns:\n",
    "    An int that results from scaling `maxval` according to `level`.\n",
    "    \"\"\"\n",
    "    return int(level * maxval / 10)\n",
    "\n",
    "\n",
    "def float_parameter(level, maxval):\n",
    "    \"\"\"Helper function to scale `val` between 0 and maxval.\n",
    "    Args:\n",
    "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
    "    maxval: Maximum value that the operation can have. This will be scaled to\n",
    "      level/PARAMETER_MAX.\n",
    "    Returns:\n",
    "    A float that results from scaling `maxval` according to `level`.\n",
    "    \"\"\"\n",
    "    return float(level) * maxval / 10.\n",
    "\n",
    "\n",
    "def sample_level(n):\n",
    "    return np.random.uniform(low=0.1, high=n)\n",
    "\n",
    "\n",
    "def autocontrast(pil_img, _):\n",
    "    return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "\n",
    "def equalize(pil_img, _):\n",
    "    return ImageOps.equalize(pil_img)\n",
    "\n",
    "\n",
    "def posterize(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), 4)\n",
    "    return ImageOps.posterize(pil_img, 4 - level)\n",
    "\n",
    "\n",
    "def rotate(pil_img, level):\n",
    "    degrees = int_parameter(sample_level(level), 30)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        degrees = -degrees\n",
    "    return pil_img.rotate(degrees, resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "def solarize(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), 256)\n",
    "    return ImageOps.solarize(pil_img, 256 - level)\n",
    "\n",
    "\n",
    "def shear_x(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform(pil_img.size,\n",
    "                           Image.AFFINE, (1, level, 0, 0, 1, 0),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "def shear_y(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform(pil_img.size,\n",
    "                           Image.AFFINE, (1, 0, 0, level, 1, 0),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "def translate_x(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), pil_img.size[0] / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform(pil_img.size,\n",
    "                           Image.AFFINE, (1, 0, level, 0, 1, 0),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "def translate_y(pil_img, level):\n",
    "    level = int_parameter(sample_level(level), pil_img.size[0] / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform(pil_img.size,\n",
    "                           Image.AFFINE, (1, 0, 0, 0, 1, level),\n",
    "                           resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "# operation that overlaps with ImageNet-C's test set\n",
    "def color(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
    "    return ImageEnhance.Color(pil_img).enhance(level)\n",
    "\n",
    "\n",
    "# operation that overlaps with ImageNet-C's test set\n",
    "def contrast(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
    "    return ImageEnhance.Contrast(pil_img).enhance(level)\n",
    "\n",
    "\n",
    "# operation that overlaps with ImageNet-C's test set\n",
    "def brightness(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
    "    return ImageEnhance.Brightness(pil_img).enhance(level)\n",
    "\n",
    "\n",
    "# operation that overlaps with ImageNet-C's test set\n",
    "def sharpness(pil_img, level):\n",
    "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
    "    return ImageEnhance.Sharpness(pil_img).enhance(level)\n",
    "\n",
    "\n",
    "augmentations = [\n",
    "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
    "    translate_x, translate_y\n",
    "]\n",
    "\n",
    "augmentations_all = [\n",
    "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
    "    translate_x, translate_y, color, contrast, brightness, sharpness\n",
    "]\n",
    "\n",
    "def normalize(image):\n",
    "    \"\"\"Normalize input image channel-wise to zero mean and unit variance.\"\"\"\n",
    "    return image - 127\n",
    "\n",
    "def apply_op(image, op, severity):\n",
    "    #   image = np.clip(image, 0, 255)\n",
    "    pil_img = Image.fromarray(image)  # Convert to PIL.Image\n",
    "    pil_img = op(pil_img, severity)\n",
    "    return np.asarray(pil_img)\n",
    "\n",
    "def augment_and_mix(image, severity=3, width=3, depth=-1, alpha=1.):\n",
    "    \"\"\"Perform AugMix augmentations and compute mixture.\n",
    "    Args:\n",
    "    image: Raw input image as float32 np.ndarray of shape (h, w, c)\n",
    "    severity: Severity of underlying augmentation operators (between 1 to 10).\n",
    "    width: Width of augmentation chain\n",
    "    depth: Depth of augmentation chain. -1 enables stochastic depth uniformly\n",
    "      from [1, 3]\n",
    "    alpha: Probability coefficient for Beta and Dirichlet distributions.\n",
    "    Returns:\n",
    "    mixed: Augmented and mixed image.\n",
    "    \"\"\"\n",
    "    ws = np.float32(\n",
    "      np.random.dirichlet([alpha] * width))\n",
    "    m = np.float32(np.random.beta(alpha, alpha))\n",
    "\n",
    "    mix = np.zeros_like(image).astype(np.float32)\n",
    "    for i in range(width):\n",
    "        image_aug = image.copy()\n",
    "        depth = depth if depth > 0 else np.random.randint(1, 4)\n",
    "        for _ in range(depth):\n",
    "            op = np.random.choice(augmentations)\n",
    "            image_aug = apply_op(image_aug, op, severity)\n",
    "        # Preprocessing commutes since all coefficients are convex\n",
    "        mix += ws[i] * image_aug\n",
    "#         mix += ws[i] * normalize(image_aug)\n",
    "\n",
    "    mixed = (1 - m) * image + m * mix\n",
    "#     mixed = (1 - m) * normalize(image) + m * mix\n",
    "    return mixed\n",
    "\n",
    "\n",
    "class RandomAugMix(ImageOnlyTransform):\n",
    "\n",
    "    def __init__(self, severity=3, width=3, depth=-1, alpha=1., always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.severity = severity\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        image = augment_and_mix(\n",
    "            image,\n",
    "            self.severity,\n",
    "            self.width,\n",
    "            self.depth,\n",
    "            self.alpha\n",
    "        )\n",
    "        return image\n",
    "\n",
    "class GridMask(DualTransform):\n",
    "    \"\"\"GridMask augmentation for image classification and object detection.\n",
    "\n",
    "    Args:\n",
    "        num_grid (int): number of grid in a row or column.\n",
    "        fill_value (int, float, lisf of int, list of float): value for dropped pixels.\n",
    "        rotate ((int, int) or int): range from which a random angle is picked. If rotate is a single int\n",
    "            an angle is picked from (-rotate, rotate). Default: (-90, 90)\n",
    "        mode (int):\n",
    "            0 - cropout a quarter of the square of each grid (left top)\n",
    "            1 - reserve a quarter of the square of each grid (left top)\n",
    "            2 - cropout 2 quarter of the square of each grid (left top & right bottom)\n",
    "\n",
    "    Targets:\n",
    "        image, mask\n",
    "\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "\n",
    "    Reference:\n",
    "    |  https://arxiv.org/abs/2001.04086\n",
    "    |  https://github.com/akuxcw/GridMask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n",
    "        super(GridMask, self).__init__(always_apply, p)\n",
    "        if isinstance(num_grid, int):\n",
    "            num_grid = (num_grid, num_grid)\n",
    "        if isinstance(rotate, int):\n",
    "            rotate = (-rotate, rotate)\n",
    "        self.num_grid = num_grid\n",
    "        self.fill_value = fill_value\n",
    "        self.rotate = rotate\n",
    "        self.mode = mode\n",
    "        self.masks = None\n",
    "        self.rand_h_max = []\n",
    "        self.rand_w_max = []\n",
    "\n",
    "    def init_masks(self, height, width):\n",
    "        if self.masks is None:\n",
    "            self.masks = []\n",
    "            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n",
    "            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n",
    "                grid_h = height / n_g\n",
    "                grid_w = width / n_g\n",
    "                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n",
    "                for i in range(n_g + 1):\n",
    "                    for j in range(n_g + 1):\n",
    "                        this_mask[\n",
    "                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n",
    "                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n",
    "                        ] = self.fill_value\n",
    "                        if self.mode == 2:\n",
    "                            this_mask[\n",
    "                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n",
    "                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n",
    "                            ] = self.fill_value\n",
    "                \n",
    "                if self.mode == 1:\n",
    "                    this_mask = 1 - this_mask\n",
    "\n",
    "                self.masks.append(this_mask)\n",
    "                self.rand_h_max.append(grid_h)\n",
    "                self.rand_w_max.append(grid_w)\n",
    "\n",
    "    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n",
    "        h, w = image.shape[:2]\n",
    "        mask = F.rotate(mask, angle) if self.rotate[1] > 0 else mask\n",
    "        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n",
    "        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n",
    "        return image\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        img = params['image']\n",
    "        height, width = img.shape[:2]\n",
    "        self.init_masks(height, width)\n",
    "\n",
    "        mid = np.random.randint(len(self.masks))\n",
    "        mask = self.masks[mid]\n",
    "        rand_h = np.random.randint(self.rand_h_max[mid])\n",
    "        rand_w = np.random.randint(self.rand_w_max[mid])\n",
    "        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n",
    "\n",
    "        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n",
    "\n",
    "    @property\n",
    "    def targets_as_params(self):\n",
    "        return ['image']\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('num_grid', 'fill_value', 'rotate', 'mode')\n",
    "\n",
    "class BengaliDatasetTrain:\n",
    "    def __init__(self, folds, img_height, img_width, mean, std):\n",
    "        df = pd.read_csv('train_folds.csv')\n",
    "        df = df[['image_id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'kfold']]\n",
    "\n",
    "        df = df[df.kfold.isin(folds)].reset_index(drop=True)\n",
    "        \n",
    "        self.image_ids = df.image_id.values\n",
    "        self.grapheme_root = df.grapheme_root.values\n",
    "        self.vowel_diacritic = df.vowel_diacritic.values\n",
    "        self.consonant_diacritic = df.consonant_diacritic.values\n",
    "\n",
    "        if len(folds) == 1:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height, img_width, always_apply=True),\n",
    "                albumentations.Normalize(mean, std, always_apply=True)\n",
    "            ])\n",
    "        else:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height, img_width, always_apply=True),\n",
    "                albumentations.ShiftScaleRotate(shift_limit=0.0625,\n",
    "                                               scale_limit=0.1, \n",
    "                                               rotate_limit=5,\n",
    "                                               p=0.9),\n",
    "#                 RandomAugMix(severity=4, width=3, alpha=1.0, p=0.75),\n",
    "                albumentations.Normalize(mean, std, always_apply=True),\n",
    "                albumentations.OneOf([\n",
    "                    GridMask(num_grid=3, mode=0, rotate=15),\n",
    "                    GridMask(num_grid=4, mode=2, rotate=15),\n",
    "                ], p=0.80)\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = joblib.load(f'image_pickles/kaggle_dataset/image_pickles/{self.image_ids[item]}.pkl')\n",
    "        image = image.reshape(137, 236).astype(float)\n",
    "        image = Image.fromarray(image).convert('RGB')\n",
    "        image = self.aug(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float),\n",
    "            'grapheme_root': torch.tensor(self.grapheme_root[item], dtype=torch.long),\n",
    "            'vowel_diacritic': torch.tensor(self.vowel_diacritic[item], dtype=torch.long),\n",
    "            'consonant_diacritic': torch.tensor(self.consonant_diacritic[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pCOcg6Fakwny",
    "outputId": "dc44ed15-3d23-4517-ee21-bef4e382dec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models.py\n",
    "\n",
    "import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(ResNet34, self).__init__()\n",
    "        if pretrained is True:\n",
    "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n",
    "        \n",
    "        self.l0 = nn.Linear(512, 168)\n",
    "        self.l1 = nn.Linear(512, 11)\n",
    "        self.l2 = nn.Linear(512, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        l0 = self.l0(x)\n",
    "        l1 = self.l1(x)\n",
    "        l2 = self.l2(x)\n",
    "        return l0, l1, l2\n",
    "\n",
    "\n",
    "class EfficientNetWrapper(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(EfficientNetWrapper, self).__init__()\n",
    "        \n",
    "        # Load imagenet pre-trained model \n",
    "        self.effNet = EfficientNet.from_pretrained('efficientnet-b3', in_channels=3)\n",
    "        \n",
    "        # Appdend output layers based on our date\n",
    "        self.fc_root = nn.Linear(in_features=1000, out_features=168)\n",
    "        self.fc_vowel = nn.Linear(in_features=1000, out_features=11)\n",
    "        self.fc_consonant = nn.Linear(in_features=1000, out_features=7)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = self.effNet(X)\n",
    "        output_root = self.fc_root(output)\n",
    "        output_vowel = self.fc_vowel(output)\n",
    "        output_consonant = self.fc_consonant(output)\n",
    "        \n",
    "        return output_root, output_vowel, output_consonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_SMG38U5k9My",
    "outputId": "c6232bc9-c2aa-47ac-a609-3cbf70ae6eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_dispatcher.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_dispatcher.py\n",
    "\n",
    "import models\n",
    "\n",
    "MODEL_DISPATCHER = {\n",
    "    'resnet34': models.ResNet34,\n",
    "    'efficientNet': models.EfficientNetWrapper\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rULmq4gcmqZN",
    "outputId": "20e6513e-7e9a-4749-f1ae-957f94258281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pytorchtools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytorchtools.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jrypLdbZlF6I",
    "outputId": "3007deb3-aa8a-4c0e-ce21-580b55eafbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "from model_dispatcher import MODEL_DISPATCHER\n",
    "from dataset import BengaliDatasetTrain\n",
    "from tqdm import tqdm\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "TRAINING_FOLDS_CSV = os.environ.get(\"TRAINING_FOLDS_CSV\")\n",
    "\n",
    "IMG_HEIGHT = int(os.environ.get(\"IMG_HEIGHT\"))\n",
    "IMG_WIDTH = int(os.environ.get(\"IMG_WIDTH\"))\n",
    "EPOCHS = int(os.environ.get(\"EPOCHS\"))\n",
    "\n",
    "TRAIN_BATCH_SIZE = int(os.environ.get(\"TRAIN_BATCH_SIZE\"))\n",
    "TEST_BATCH_SIZE = int(os.environ.get(\"TEST_BATCH_SIZE\"))\n",
    "\n",
    "MODEL_MEAN = ast.literal_eval(os.environ.get(\"MODEL_MEAN\"))\n",
    "MODEL_STD = ast.literal_eval(os.environ.get(\"MODEL_STD\"))\n",
    "\n",
    "TRAINING_FOLDS = ast.literal_eval(os.environ.get(\"TRAINING_FOLDS\"))\n",
    "VALIDATION_FOLDS = ast.literal_eval(os.environ.get(\"VALIDATION_FOLDS\"))\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\")\n",
    "\n",
    "\n",
    "\n",
    "def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    \n",
    "    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n",
    "\n",
    "    y = y.cpu().numpy()\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[:, 2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, total: {final_score}, y {y.shape}')\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    o1, o2, o3 = outputs\n",
    "    t1, t2, t3 = targets\n",
    "    l1 = nn.CrossEntropyLoss()(o1, t1)\n",
    "    l2 = nn.CrossEntropyLoss()(o2, t2)\n",
    "    l3 = nn.CrossEntropyLoss()(o3, t3)\n",
    "    return (l1 + l2 + l3) / 3\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset, data_loader, model, optimizer):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    counter = 0\n",
    "    final_outputs = []\n",
    "    final_targets = []\n",
    "\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)/data_loader.batch_size)):\n",
    "        counter = counter + 1\n",
    "        image = d[\"image\"]\n",
    "        grapheme_root = d[\"grapheme_root\"]\n",
    "        vowel_diacritic = d[\"vowel_diacritic\"]\n",
    "        consonant_diacritic = d[\"consonant_diacritic\"]\n",
    "\n",
    "        image = image.to(DEVICE, dtype=torch.float)\n",
    "        grapheme_root = grapheme_root.to(DEVICE, dtype=torch.long)\n",
    "        vowel_diacritic = vowel_diacritic.to(DEVICE, dtype=torch.long)\n",
    "        consonant_diacritic = consonant_diacritic.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        final_loss += loss\n",
    "\n",
    "        o1, o2, o3 = outputs\n",
    "        t1, t2, t3 = targets\n",
    "        final_outputs.append(torch.cat((o1,o2,o3), dim=1))\n",
    "        final_targets.append(torch.stack((t1,t2,t3), dim=1))\n",
    "\n",
    "    final_outputs = torch.cat(final_outputs)\n",
    "    final_targets = torch.cat(final_targets)\n",
    "\n",
    "    print(\"=================Train=================\")\n",
    "    macro_recall_score = macro_recall(final_outputs, final_targets)\n",
    "    \n",
    "    return final_loss/counter , macro_recall_score\n",
    "\n",
    "\n",
    "def evaluate(dataset, data_loader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        final_loss = 0\n",
    "        counter = 0\n",
    "        final_outputs = []\n",
    "        final_targets = []\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)/data_loader.batch_size)):\n",
    "            counter = counter + 1\n",
    "            image = d[\"image\"]\n",
    "            grapheme_root = d[\"grapheme_root\"]\n",
    "            vowel_diacritic = d[\"vowel_diacritic\"]\n",
    "            consonant_diacritic = d[\"consonant_diacritic\"]\n",
    "\n",
    "            image = image.to(DEVICE, dtype=torch.float)\n",
    "            grapheme_root = grapheme_root.to(DEVICE, dtype=torch.long)\n",
    "            vowel_diacritic = vowel_diacritic.to(DEVICE, dtype=torch.long)\n",
    "            consonant_diacritic = consonant_diacritic.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "            outputs = model(image)\n",
    "            targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            final_loss += loss\n",
    "\n",
    "            o1, o2, o3 = outputs\n",
    "            t1, t2, t3 = targets\n",
    "\n",
    "            final_outputs.append(torch.cat((o1,o2,o3), dim=1))\n",
    "            final_targets.append(torch.stack((t1,t2,t3), dim=1))\n",
    "        \n",
    "        final_outputs = torch.cat(final_outputs)\n",
    "        final_targets = torch.cat(final_targets)\n",
    "\n",
    "        print(\"=================Validation=================\")\n",
    "        macro_recall_score = macro_recall(final_outputs, final_targets)\n",
    "\n",
    "    return final_loss/counter , macro_recall_score\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = MODEL_DISPATCHER[BASE_MODEL](pretrained=True)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    train_dataset = BengaliDatasetTrain(\n",
    "        folds=TRAINING_FOLDS,\n",
    "        img_height = IMG_HEIGHT,\n",
    "        img_width = IMG_WIDTH,\n",
    "        mean = MODEL_MEAN,\n",
    "        std = MODEL_STD\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size= TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = BengaliDatasetTrain(\n",
    "        folds=VALIDATION_FOLDS,\n",
    "        img_height = IMG_HEIGHT,\n",
    "        img_width = IMG_WIDTH,\n",
    "        mean = MODEL_MEAN,\n",
    "        std = MODEL_STD\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size= TEST_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                            mode=\"min\", \n",
    "                                                            patience=5, \n",
    "                                                            factor=0.3,verbose=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    best_score = -1\n",
    "\n",
    "    print(\"FOLD : \", VALIDATION_FOLDS[0] )\n",
    "    \n",
    "    val_scores = []\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss, train_score = train(train_dataset,train_loader, model, optimizer)\n",
    "        val_loss, val_score = evaluate(valid_dataset, valid_loader, model)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            torch.save(model.state_dict(), \"{}_fold{}.pth\".format(BASE_MODEL, VALIDATION_FOLDS[0]))\n",
    "\n",
    "        epoch_len = len(str(EPOCHS))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{EPOCHS:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'train_score: {train_score:.5f} ' +\n",
    "                     f'valid_loss: {val_loss:.5f} ' +\n",
    "                     f'valid_score: {val_score:.5f}'\n",
    "                    )\n",
    "        val_scores.append(val_score)\n",
    "        print(print_msg)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, len(val_scores)), val_scores, label='val_scores')\n",
    "\n",
    "    plt.title('Recall Score')\n",
    "    plt.xlabel('# of epochs')\n",
    "    plt.ylabel('Recall Score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "paWyDwdQmvuT",
    "outputId": "7c0b2691-5675-4d1a-bdb0-75e8b46e8f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "export IMG_HEIGHT=137\n",
    "export IMG_WIDTH=236\n",
    "export EPOCHS=40\n",
    "export TRAIN_BATCH_SIZE=128\n",
    "export TEST_BATCH_SIZE=64\n",
    "export MODEL_MEAN=\"(0.485, 0.456, 0.406)\"\n",
    "export MODEL_STD=\"(0.229, 0.224, 0.225)\"\n",
    "export BASE_MODEL=\"efficientNet\"\n",
    "export TRAINING_FOLDS_CSV=\"train_folds.csv\"\n",
    "\n",
    "\n",
    "export TRAINING_FOLDS=\"(0,1,2,3)\"\n",
    "export VALIDATION_FOLDS=\"(4,)\"\n",
    "python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(0,1,2,4)\"\n",
    "export VALIDATION_FOLDS=\"(3,)\"\n",
    "python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(0,1,3,4)\"\n",
    "export VALIDATION_FOLDS=\"(2,)\"\n",
    "python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(0,2,3,4)\"\n",
    "export VALIDATION_FOLDS=\"(1,)\"\n",
    "python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(1,2,3,4)\"\n",
    "export VALIDATION_FOLDS=\"(0,)\"\n",
    "python3 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "SOZ31JlHnG-S",
    "outputId": "6123f947-3c8a-4c54-dd5d-17e67e1209c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|████▍                                   | 140/1255 [01:50<14:53,  1.25it/s]"
     ]
    }
   ],
   "source": [
    "!sh run.sh > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMYAYzQiwcZG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bengaliai-no-augmentations-fold1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
