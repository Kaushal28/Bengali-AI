{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2htaYO-fjk9R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: iterative-stratification in ./.local/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from iterative-stratification) (0.22)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/anaconda3/lib/python3.7/site-packages (from iterative-stratification) (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/anaconda3/lib/python3.7/site-packages (from iterative-stratification) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->iterative-stratification) (0.14.1)\n",
      "Requirement already up-to-date: kaggle in ./.local/lib/python3.7/site-packages (1.5.6)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from kaggle) (4.40.2)\n",
      "Requirement already satisfied, skipping upgrade: python-slugify in ./.local/lib/python3.7/site-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/anaconda3/lib/python3.7/site-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/anaconda3/lib/python3.7/site-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/anaconda3/lib/python3.7/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/anaconda3/lib/python3.7/site-packages (from kaggle) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from kaggle) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in ./.local/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->kaggle) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already up-to-date: pretrainedmodels in ./.local/lib/python3.7/site-packages (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: torch in ./.local/lib/python3.7/site-packages (from pretrainedmodels) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in ./.local/lib/python3.7/site-packages (from pretrainedmodels) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: munch in ./.local/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from pretrainedmodels) (4.40.2)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /opt/anaconda3/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (6.2.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/anaconda3/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/anaconda3/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (1.17.4)\n",
      "Requirement already up-to-date: efficientnet-pytorch in ./.local/lib/python3.7/site-packages (0.6.3)\n",
      "Requirement already satisfied, skipping upgrade: torch in ./.local/lib/python3.7/site-packages (from efficientnet-pytorch) (1.4.0)\n",
      "Requirement already up-to-date: albumentations in ./.local/lib/python3.7/site-packages (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: imgaug<0.2.7,>=0.2.5 in ./.local/lib/python3.7/site-packages (from albumentations) (0.2.6)\n",
      "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in ./.local/lib/python3.7/site-packages (from albumentations) (4.2.0.32)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/anaconda3/lib/python3.7/site-packages (from albumentations) (1.3.2)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/anaconda3/lib/python3.7/site-packages (from albumentations) (5.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /opt/anaconda3/lib/python3.7/site-packages (from albumentations) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /opt/anaconda3/lib/python3.7/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.15.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/anaconda3/lib/python3.7/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (6.2.1)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.5)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user iterative-stratification --upgrade\n",
    "!pip install --user kaggle --upgrade\n",
    "!pip install --user pretrainedmodels --upgrade\n",
    "!pip install --user --upgrade efficientnet-pytorch\n",
    "!pip install --user --upgrade albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_-nJiKmJjz0J",
    "outputId": "8b98601e-dfc5-4b93-b2da-f94d0fd38101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/kaushal28shah/.kaggle/’: File exists\n",
      "mv: cannot stat 'kaggle.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!sudo mkdir /home/kaushal28shah/.kaggle/\n",
    "!sudo mv kaggle.json /home/kaushal28shah/.kaggle/\n",
    "!sudo chmod 777 /home/kaushal28shah/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oBiFoxQHj70v",
    "outputId": "61a7f872-8784-48e8-84bc-d191f6a82691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/kaushal28shah/.kaggle/kaggle.json'\n",
      "Downloading bengaliai-image-pickles.zip to /home/kaushal28shah\n",
      " 99%|██████████████████████████████████████▌| 1.93G/1.95G [00:18<00:00, 204MB/s]\n",
      "100%|███████████████████████████████████████| 1.95G/1.95G [00:18<00:00, 110MB/s]\n"
     ]
    }
   ],
   "source": [
    "!/home/kaushal28shah/.local/bin/kaggle datasets download -d bengaliai-image-pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_nQSsYSMmvrG",
    "outputId": "e29482ac-6111-4a95-8ae0-a94355adad60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/kaushal28shah/.kaggle/kaggle.json'\n",
      "Downloading efficientnet-pytorch.zip to /home/kaushal28shah\n",
      " 98%|███████████████████████████████████████ | 673M/688M [00:14<00:00, 29.3MB/s]\n",
      "100%|████████████████████████████████████████| 688M/688M [00:14<00:00, 49.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!/home/kaushal28shah/.local/bin/kaggle datasets download -d hmendonca/efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-21+deb9u2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "XpETHpwIoBGE",
    "outputId": "4e8497df-db26-4218-eb45-6eb9d27964c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  efficientnet-pytorch.zip\n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/.gitignore  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/LICENSE  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/README.md  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/efficientnet_pytorch/__init__.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/efficientnet_pytorch/model.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/efficientnet_pytorch/utils.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/imagenet/README.md  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/imagenet/data/README.md  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/imagenet/main.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/simple/check.ipynb  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/simple/example.ipynb  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/simple/img.jpg  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/simple/img2.jpg  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/examples/simple/labels_map.txt  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/setup.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/README.md  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/download.sh  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/load_tf_weights.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/original_tf/__init__.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/original_tf/efficientnet_builder.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/original_tf/efficientnet_model.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/original_tf/eval_ckpt_main.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/original_tf/preprocessing.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/convert_tf_to_pt/original_tf/utils.py  \n",
      "  inflating: EfficientNet-PyTorch/EfficientNet-PyTorch-master/tf_to_pytorch/pretrained_tensorflow/download.sh  \n",
      "  inflating: efficientnet-b0-08094119.pth  \n",
      "  inflating: efficientnet-b1-dbc7070a.pth  \n",
      "  inflating: efficientnet-b2-27687264.pth  \n",
      "  inflating: efficientnet-b3-c8376fa2.pth  \n",
      "  inflating: efficientnet-b4-e116e8b3.pth  \n",
      "  inflating: efficientnet-b5-586e6cc6.pth  \n",
      "  inflating: efficientnet-b6-c76e70fd.pth  \n",
      "  inflating: efficientnet-b7-dcc49843.pth  \n"
     ]
    }
   ],
   "source": [
    "!sudo unzip efficientnet-pytorch.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2xNzT0hoSez"
   },
   "outputs": [],
   "source": [
    "!sudo mkdir -p /root/.cache/torch/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZLHh54gmuFm"
   },
   "outputs": [],
   "source": [
    "!sudo cp efficientnet-b1-dbc7070a.pth /root/.cache/torch/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "M1WtEReAkAXZ",
    "outputId": "f42809fd-3c67-4cef-8090-65cb8a1b1c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 24 ms, total: 328 ms\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!unzip -qq bengaliai-image-pickles.zip\n",
    "!rm -rf bengaliai-image-pickles.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4z6yoETVkE1y",
    "outputId": "9638f832-3b81-4061-a093-9833aa1b7dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.augmentations import functional as F\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "class GridMask(DualTransform):\n",
    "    \"\"\"GridMask augmentation for image classification and object detection.\n",
    "\n",
    "    Args:\n",
    "        num_grid (int): number of grid in a row or column.\n",
    "        fill_value (int, float, lisf of int, list of float): value for dropped pixels.\n",
    "        rotate ((int, int) or int): range from which a random angle is picked. If rotate is a single int\n",
    "            an angle is picked from (-rotate, rotate). Default: (-90, 90)\n",
    "        mode (int):\n",
    "            0 - cropout a quarter of the square of each grid (left top)\n",
    "            1 - reserve a quarter of the square of each grid (left top)\n",
    "            2 - cropout 2 quarter of the square of each grid (left top & right bottom)\n",
    "\n",
    "    Targets:\n",
    "        image, mask\n",
    "\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "\n",
    "    Reference:\n",
    "    |  https://arxiv.org/abs/2001.04086\n",
    "    |  https://github.com/akuxcw/GridMask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n",
    "        super(GridMask, self).__init__(always_apply, p)\n",
    "        if isinstance(num_grid, int):\n",
    "            num_grid = (num_grid, num_grid)\n",
    "        if isinstance(rotate, int):\n",
    "            rotate = (-rotate, rotate)\n",
    "        self.num_grid = num_grid\n",
    "        self.fill_value = fill_value\n",
    "        self.rotate = rotate\n",
    "        self.mode = mode\n",
    "        self.masks = None\n",
    "        self.rand_h_max = []\n",
    "        self.rand_w_max = []\n",
    "\n",
    "    def init_masks(self, height, width):\n",
    "        if self.masks is None:\n",
    "            self.masks = []\n",
    "            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n",
    "            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n",
    "                grid_h = height / n_g\n",
    "                grid_w = width / n_g\n",
    "                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n",
    "                for i in range(n_g + 1):\n",
    "                    for j in range(n_g + 1):\n",
    "                        this_mask[\n",
    "                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n",
    "                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n",
    "                        ] = self.fill_value\n",
    "                        if self.mode == 2:\n",
    "                            this_mask[\n",
    "                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n",
    "                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n",
    "                            ] = self.fill_value\n",
    "                \n",
    "                if self.mode == 1:\n",
    "                    this_mask = 1 - this_mask\n",
    "\n",
    "                self.masks.append(this_mask)\n",
    "                self.rand_h_max.append(grid_h)\n",
    "                self.rand_w_max.append(grid_w)\n",
    "\n",
    "    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n",
    "        h, w = image.shape[:2]\n",
    "        mask = F.rotate(mask, angle) if self.rotate[1] > 0 else mask\n",
    "        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n",
    "        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n",
    "        return image\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        img = params['image']\n",
    "        height, width = img.shape[:2]\n",
    "        self.init_masks(height, width)\n",
    "\n",
    "        mid = np.random.randint(len(self.masks))\n",
    "        mask = self.masks[mid]\n",
    "        rand_h = np.random.randint(self.rand_h_max[mid])\n",
    "        rand_w = np.random.randint(self.rand_w_max[mid])\n",
    "        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n",
    "\n",
    "        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n",
    "\n",
    "    @property\n",
    "    def targets_as_params(self):\n",
    "        return ['image']\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('num_grid', 'fill_value', 'rotate', 'mode')\n",
    "\n",
    "class BengaliDatasetTrain:\n",
    "    def __init__(self, folds, img_height, img_width, mean, std):\n",
    "        df = pd.read_csv('train_folds.csv')\n",
    "        df = df[['image_id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'kfold']]\n",
    "\n",
    "        df = df[df.kfold.isin(folds)].reset_index(drop=True)\n",
    "        \n",
    "        self.image_ids = df.image_id.values\n",
    "        self.grapheme_root = df.grapheme_root.values\n",
    "        self.vowel_diacritic = df.vowel_diacritic.values\n",
    "        self.consonant_diacritic = df.consonant_diacritic.values\n",
    "\n",
    "        if len(folds) == 1:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height, img_width, always_apply=True),\n",
    "                albumentations.Normalize(mean, std, always_apply=True)\n",
    "            ])\n",
    "        else:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height, img_width, always_apply=True),\n",
    "                albumentations.ShiftScaleRotate(shift_limit=0.0625,\n",
    "                                               scale_limit=0.1, \n",
    "                                               rotate_limit=5,\n",
    "                                               p=0.9),\n",
    "                albumentations.Normalize(mean, std, always_apply=True),\n",
    "                albumentations.OneOf([\n",
    "                    GridMask(num_grid=3, mode=0, rotate=15),\n",
    "                    GridMask(num_grid=3, mode=2, rotate=15),\n",
    "                ], p=0.75)\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image = joblib.load(f'image_pickles/kaggle_dataset/image_pickles/{self.image_ids[item]}.pkl')\n",
    "        image = image.reshape(137, 236).astype(float)\n",
    "        image = Image.fromarray(image).convert('RGB')\n",
    "        image = self.aug(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float),\n",
    "            'grapheme_root': torch.tensor(self.grapheme_root[item], dtype=torch.long),\n",
    "            'vowel_diacritic': torch.tensor(self.vowel_diacritic[item], dtype=torch.long),\n",
    "            'consonant_diacritic': torch.tensor(self.consonant_diacritic[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pCOcg6Fakwny",
    "outputId": "ad3e7af1-9e7f-452c-a090-dee3e73133dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models.py\n",
    "\n",
    "import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def custom_load_pretrained_weights(model, model_name, load_fc=True, advprop=False):\n",
    "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "    # AutoAugment or Advprop (different preprocessing)\n",
    "    url_map_ = url_map_advprop if advprop else url_map\n",
    "    state_dict = torch.load('efficientnet-b1-dbc7070a.pth', map_location='cpu')\n",
    "    if load_fc:\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        state_dict.pop('_fc.weight')\n",
    "        state_dict.pop('_fc.bias')\n",
    "        res = model.load_state_dict(state_dict, strict=False)\n",
    "        assert set(res.missing_keys) == set(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))\n",
    "\n",
    "from efficientnet_pytorch import utils\n",
    "utils.load_pretrained_weights.__code__ = custom_load_pretrained_weights.__code__\n",
    "\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(ResNet34, self).__init__()\n",
    "        if pretrained is True:\n",
    "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n",
    "        \n",
    "        self.l0 = nn.Linear(512, 168)\n",
    "        self.l1 = nn.Linear(512, 11)\n",
    "        self.l2 = nn.Linear(512, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        l0 = self.l0(x)\n",
    "        l1 = self.l1(x)\n",
    "        l2 = self.l2(x)\n",
    "        return l0, l1, l2\n",
    "\n",
    "\n",
    "class EfficientNetWrapper(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(EfficientNetWrapper, self).__init__()\n",
    "        \n",
    "        # Load imagenet pre-trained model \n",
    "        self.effNet = EfficientNet.from_pretrained('efficientnet-b1', in_channels=3)\n",
    "        \n",
    "        # Appdend output layers based on our date\n",
    "        self.fc_root = nn.Linear(in_features=1000, out_features=168)\n",
    "        self.fc_vowel = nn.Linear(in_features=1000, out_features=11)\n",
    "        self.fc_consonant = nn.Linear(in_features=1000, out_features=7)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = self.effNet(X)\n",
    "        output_root = self.fc_root(output)\n",
    "        output_vowel = self.fc_vowel(output)\n",
    "        output_consonant = self.fc_consonant(output)\n",
    "        \n",
    "        return output_root, output_vowel, output_consonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_SMG38U5k9My",
    "outputId": "79e99a06-11ca-410f-a66c-5676ce967e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_dispatcher.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_dispatcher.py\n",
    "\n",
    "import models\n",
    "\n",
    "MODEL_DISPATCHER = {\n",
    "    'resnet34': models.ResNet34,\n",
    "    'efficientNet': models.EfficientNetWrapper\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rULmq4gcmqZN",
    "outputId": "f07a0877-4d74-48c9-985a-b8f599dc40b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytorchtools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytorchtools.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jrypLdbZlF6I",
    "outputId": "241f011b-12a7-49fb-b779-292d1ba40a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "from model_dispatcher import MODEL_DISPATCHER\n",
    "from dataset import BengaliDatasetTrain\n",
    "from tqdm import tqdm\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "TRAINING_FOLDS_CSV = os.environ.get(\"TRAINING_FOLDS_CSV\")\n",
    "\n",
    "IMG_HEIGHT = int(os.environ.get(\"IMG_HEIGHT\"))\n",
    "IMG_WIDTH = int(os.environ.get(\"IMG_WIDTH\"))\n",
    "EPOCHS = int(os.environ.get(\"EPOCHS\"))\n",
    "\n",
    "TRAIN_BATCH_SIZE = int(os.environ.get(\"TRAIN_BATCH_SIZE\"))\n",
    "TEST_BATCH_SIZE = int(os.environ.get(\"TEST_BATCH_SIZE\"))\n",
    "\n",
    "MODEL_MEAN = ast.literal_eval(os.environ.get(\"MODEL_MEAN\"))\n",
    "MODEL_STD = ast.literal_eval(os.environ.get(\"MODEL_STD\"))\n",
    "\n",
    "TRAINING_FOLDS = ast.literal_eval(os.environ.get(\"TRAINING_FOLDS\"))\n",
    "VALIDATION_FOLDS = ast.literal_eval(os.environ.get(\"VALIDATION_FOLDS\"))\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\")\n",
    "\n",
    "\n",
    "\n",
    "def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    \n",
    "    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n",
    "\n",
    "    y = y.cpu().numpy()\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[:, 2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, total: {final_score}, y {y.shape}')\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    o1, o2, o3 = outputs\n",
    "    t1, t2, t3 = targets\n",
    "    l1 = nn.CrossEntropyLoss()(o1, t1)\n",
    "    l2 = nn.CrossEntropyLoss()(o2, t2)\n",
    "    l3 = nn.CrossEntropyLoss()(o3, t3)\n",
    "    return (l1 + l2 + l3) / 3\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset, data_loader, model, optimizer):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    counter = 0\n",
    "    final_outputs = []\n",
    "    final_targets = []\n",
    "\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)/data_loader.batch_size)):\n",
    "        counter = counter + 1\n",
    "        image = d[\"image\"]\n",
    "        grapheme_root = d[\"grapheme_root\"]\n",
    "        vowel_diacritic = d[\"vowel_diacritic\"]\n",
    "        consonant_diacritic = d[\"consonant_diacritic\"]\n",
    "\n",
    "        image = image.to(DEVICE, dtype=torch.float)\n",
    "        grapheme_root = grapheme_root.to(DEVICE, dtype=torch.long)\n",
    "        vowel_diacritic = vowel_diacritic.to(DEVICE, dtype=torch.long)\n",
    "        consonant_diacritic = consonant_diacritic.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        final_loss += loss\n",
    "\n",
    "        o1, o2, o3 = outputs\n",
    "        t1, t2, t3 = targets\n",
    "        final_outputs.append(torch.cat((o1,o2,o3), dim=1))\n",
    "        final_targets.append(torch.stack((t1,t2,t3), dim=1))\n",
    "\n",
    "    final_outputs = torch.cat(final_outputs)\n",
    "    final_targets = torch.cat(final_targets)\n",
    "\n",
    "    print(\"=================Train=================\")\n",
    "    macro_recall_score = macro_recall(final_outputs, final_targets)\n",
    "    \n",
    "    return final_loss/counter , macro_recall_score\n",
    "\n",
    "\n",
    "def evaluate(dataset, data_loader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        final_loss = 0\n",
    "        counter = 0\n",
    "        final_outputs = []\n",
    "        final_targets = []\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)/data_loader.batch_size)):\n",
    "            counter = counter + 1\n",
    "            image = d[\"image\"]\n",
    "            grapheme_root = d[\"grapheme_root\"]\n",
    "            vowel_diacritic = d[\"vowel_diacritic\"]\n",
    "            consonant_diacritic = d[\"consonant_diacritic\"]\n",
    "\n",
    "            image = image.to(DEVICE, dtype=torch.float)\n",
    "            grapheme_root = grapheme_root.to(DEVICE, dtype=torch.long)\n",
    "            vowel_diacritic = vowel_diacritic.to(DEVICE, dtype=torch.long)\n",
    "            consonant_diacritic = consonant_diacritic.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "            outputs = model(image)\n",
    "            targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            final_loss += loss\n",
    "\n",
    "            o1, o2, o3 = outputs\n",
    "            t1, t2, t3 = targets\n",
    "\n",
    "            final_outputs.append(torch.cat((o1,o2,o3), dim=1))\n",
    "            final_targets.append(torch.stack((t1,t2,t3), dim=1))\n",
    "        \n",
    "        final_outputs = torch.cat(final_outputs)\n",
    "        final_targets = torch.cat(final_targets)\n",
    "\n",
    "        print(\"=================Validation=================\")\n",
    "        macro_recall_score = macro_recall(final_outputs, final_targets)\n",
    "\n",
    "    return final_loss/counter , macro_recall_score\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = MODEL_DISPATCHER[BASE_MODEL](pretrained=True)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    train_dataset = BengaliDatasetTrain(\n",
    "        folds=TRAINING_FOLDS,\n",
    "        img_height = IMG_HEIGHT,\n",
    "        img_width = IMG_WIDTH,\n",
    "        mean = MODEL_MEAN,\n",
    "        std = MODEL_STD\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size= TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = BengaliDatasetTrain(\n",
    "        folds=VALIDATION_FOLDS,\n",
    "        img_height = IMG_HEIGHT,\n",
    "        img_width = IMG_WIDTH,\n",
    "        mean = MODEL_MEAN,\n",
    "        std = MODEL_STD\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size= TEST_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                            mode=\"min\", \n",
    "                                                            patience=5, \n",
    "                                                            factor=0.3,verbose=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    best_score = -1\n",
    "\n",
    "    print(\"FOLD : \", VALIDATION_FOLDS[0] )\n",
    "    \n",
    "    val_scores = []\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss, train_score = train(train_dataset,train_loader, model, optimizer)\n",
    "        val_loss, val_score = evaluate(valid_dataset, valid_loader, model)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            torch.save(model.state_dict(), \"{}_fold{}.pth\".format(BASE_MODEL, VALIDATION_FOLDS[0]))\n",
    "\n",
    "        epoch_len = len(str(EPOCHS))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{EPOCHS:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'train_score: {train_score:.5f} ' +\n",
    "                     f'valid_loss: {val_loss:.5f} ' +\n",
    "                     f'valid_score: {val_score:.5f}'\n",
    "                    )\n",
    "        val_scores.append(val_score)\n",
    "        print(print_msg)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, len(val_scores)), val_scores, label='val_scores')\n",
    "\n",
    "    plt.title('Recall Score')\n",
    "    plt.xlabel('# of epochs')\n",
    "    plt.ylabel('Recall Score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "paWyDwdQmvuT",
    "outputId": "d1a8c0e8-d1bb-4b3b-b802-3b165e564985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "export IMG_HEIGHT=128\n",
    "export IMG_WIDTH=128\n",
    "export EPOCHS=30\n",
    "export TRAIN_BATCH_SIZE=256\n",
    "export TEST_BATCH_SIZE=64\n",
    "export MODEL_MEAN=\"(0.485, 0.456, 0.406)\"\n",
    "export MODEL_STD=\"(0.229, 0.224, 0.225)\"\n",
    "export BASE_MODEL=\"efficientNet\"\n",
    "export TRAINING_FOLDS_CSV=\"train_folds.csv\"\n",
    "\n",
    "\n",
    "# export TRAINING_FOLDS=\"(0,1,2,3)\"\n",
    "# export VALIDATION_FOLDS=\"(4,)\"\n",
    "# python3 train.py\n",
    "\n",
    "# export TRAINING_FOLDS=\"(0,1,2,4)\"\n",
    "# export VALIDATION_FOLDS=\"(3,)\"\n",
    "# python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(0,1,3,4)\"\n",
    "export VALIDATION_FOLDS=\"(2,)\"\n",
    "python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(0,2,3,4)\"\n",
    "export VALIDATION_FOLDS=\"(1,)\"\n",
    "python3 train.py\n",
    "\n",
    "export TRAINING_FOLDS=\"(1,2,3,4)\"\n",
    "export VALIDATION_FOLDS=\"(0,)\"\n",
    "python3 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "colab_type": "code",
    "id": "SOZ31JlHnG-S",
    "outputId": "48f06178-c0e1-4c27-cd24-5c7c01a29493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n",
      "FOLD :  2\n",
      "628it [03:55,  2.67it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.07566536568874002, vowel 0.5568785346825881, consonant 0.49880796445499126, total: 0.3017543076287648, y (160672, 3)\n",
      "628it [00:41, 15.02it/s]                                                        \n",
      "=================Validation=================\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "recall: grapheme 0.4119072197794174, vowel 0.8483057212288443, consonant 0.8507142062840873, total: 0.6307085917679416, y (40168, 3)\n",
      "[ 1/30] train_loss: 2.06063 train_score: 0.30175 valid_loss: 0.99065 valid_score: 0.63071\n",
      "Validation loss decreased (inf --> 0.990647).  Saving model ...\n",
      "628it [03:25,  3.05it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.4068311290117662, vowel 0.8068005567875404, consonant 0.7954909598364701, total: 0.6039884436618856, y (160672, 3)\n",
      "628it [00:33, 18.89it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.7257519001009731, vowel 0.9253048157509657, consonant 0.9253868716316956, total: 0.8255488718961519, y (40168, 3)\n",
      "[ 2/30] train_loss: 1.04319 train_score: 0.60399 valid_loss: 0.41924 valid_score: 0.82555\n",
      "Validation loss decreased (0.990647 --> 0.419239).  Saving model ...\n",
      "628it [03:36,  2.90it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.5850525148096543, vowel 0.8720020976899613, consonant 0.8594466266244555, total: 0.7253884384834314, y (160672, 3)\n",
      "628it [00:32, 19.08it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.8143125033126039, vowel 0.9507975593215794, consonant 0.9418682657972376, total: 0.8803227079360062, y (40168, 3)\n",
      "[ 3/30] train_loss: 0.69699 train_score: 0.72539 valid_loss: 0.29722 valid_score: 0.88032\n",
      "Validation loss decreased (0.419239 --> 0.297216).  Saving model ...\n",
      "628it [03:30,  2.98it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.6667254192799343, vowel 0.8959960278392409, consonant 0.8855829306928298, total: 0.7787574492729848, y (160672, 3)\n",
      "628it [00:32, 19.53it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.8552177479834759, vowel 0.9568107627185666, consonant 0.9533705176066475, total: 0.9051541940730414, y (40168, 3)\n",
      "[ 4/30] train_loss: 0.55502 train_score: 0.77876 valid_loss: 0.23009 valid_score: 0.90515\n",
      "Validation loss decreased (0.297216 --> 0.230094).  Saving model ...\n",
      "628it [03:27,  3.02it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.7109769074455844, vowel 0.9114438514545035, consonant 0.8995119359507628, total: 0.8082274005741088, y (160672, 3)\n",
      "628it [00:31, 19.67it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.8759745524283034, vowel 0.9596848015820124, consonant 0.9526647410259478, total: 0.9160746618661417, y (40168, 3)\n",
      "[ 5/30] train_loss: 0.47710 train_score: 0.80823 valid_loss: 0.19995 valid_score: 0.91607\n",
      "Validation loss decreased (0.230094 --> 0.199955).  Saving model ...\n",
      "628it [03:33,  2.95it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.7414046260264635, vowel 0.9209252141168359, consonant 0.9084177493205514, total: 0.8280380538725786, y (160672, 3)\n",
      "628it [00:31, 20.00it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.8847900773512644, vowel 0.9658354647546994, consonant 0.9615444674633481, total: 0.9242400217301441, y (40168, 3)\n",
      "[ 6/30] train_loss: 0.42345 train_score: 0.82804 valid_loss: 0.18017 valid_score: 0.92424\n",
      "Validation loss decreased (0.199955 --> 0.180172).  Saving model ...\n",
      "628it [03:31,  2.97it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.7668266462531316, vowel 0.9294748672926256, consonant 0.9185204746960659, total: 0.8454121586237388, y (160672, 3)\n",
      "628it [00:31, 19.76it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9022690397553566, vowel 0.9696149768736753, consonant 0.9645688834210625, total: 0.9346804849513627, y (40168, 3)\n",
      "[ 7/30] train_loss: 0.38594 train_score: 0.84541 valid_loss: 0.16405 valid_score: 0.93468\n",
      "Validation loss decreased (0.180172 --> 0.164047).  Saving model ...\n",
      "628it [03:32,  2.96it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.7866408189670684, vowel 0.9347557580596461, consonant 0.9235056764138461, total: 0.8578857681019072, y (160672, 3)\n",
      "628it [00:31, 20.02it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9049628808877694, vowel 0.9708885253042182, consonant 0.9688715880493017, total: 0.9374214687822647, y (40168, 3)\n",
      "[ 8/30] train_loss: 0.35559 train_score: 0.85789 valid_loss: 0.15678 valid_score: 0.93742\n",
      "Validation loss decreased (0.164047 --> 0.156779).  Saving model ...\n",
      "628it [03:30,  2.98it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.7993342545374917, vowel 0.9396264319591119, consonant 0.925837610542294, total: 0.8660331378940974, y (160672, 3)\n",
      "628it [00:31, 19.87it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9157891820191806, vowel 0.9740195893760027, consonant 0.9696596311175048, total: 0.9438143961329672, y (40168, 3)\n",
      "[ 9/30] train_loss: 0.33056 train_score: 0.86603 valid_loss: 0.14692 valid_score: 0.94381\n",
      "Validation loss decreased (0.156779 --> 0.146919).  Saving model ...\n",
      "628it [03:31,  2.97it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8147096962949625, vowel 0.9422537240810004, consonant 0.9313988413022993, total: 0.8757679894933061, y (160672, 3)\n",
      "628it [00:31, 20.09it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.915533978752763, vowel 0.9694629564174182, consonant 0.966767188713903, total: 0.9418245256592118, y (40168, 3)\n",
      "[10/30] train_loss: 0.31284 train_score: 0.87577 valid_loss: 0.14429 valid_score: 0.94182\n",
      "Validation loss decreased (0.146919 --> 0.144293).  Saving model ...\n",
      "628it [03:29,  2.99it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8204219572184919, vowel 0.9452609305471316, consonant 0.9323594355293444, total: 0.879616070128365, y (160672, 3)\n",
      "628it [00:31, 20.01it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9178168735756939, vowel 0.9729402131796437, consonant 0.9740925693966466, total: 0.9456666324319195, y (40168, 3)\n",
      "[11/30] train_loss: 0.29552 train_score: 0.87962 valid_loss: 0.13131 valid_score: 0.94567\n",
      "Validation loss decreased (0.144293 --> 0.131306).  Saving model ...\n",
      "628it [03:29,  3.00it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8295713071616801, vowel 0.9476951064683927, consonant 0.9370035239996062, total: 0.8859603111978398, y (160672, 3)\n",
      "628it [00:30, 20.26it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9216472246351968, vowel 0.9749330018623613, consonant 0.9714028978347394, total: 0.9474075872418736, y (40168, 3)\n",
      "[12/30] train_loss: 0.27997 train_score: 0.88596 valid_loss: 0.12777 valid_score: 0.94741\n",
      "Validation loss decreased (0.131306 --> 0.127770).  Saving model ...\n",
      "628it [03:26,  3.05it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8369644454490504, vowel 0.9510647651215499, consonant 0.9424420267470357, total: 0.8918589206916716, y (160672, 3)\n",
      "628it [00:31, 20.24it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.926712501607914, vowel 0.9781002221740921, consonant 0.9710742030604294, total: 0.9506498571125874, y (40168, 3)\n",
      "[13/30] train_loss: 0.26899 train_score: 0.89186 valid_loss: 0.12235 valid_score: 0.95065\n",
      "Validation loss decreased (0.127770 --> 0.122345).  Saving model ...\n",
      "628it [03:29,  3.00it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8451102270027369, vowel 0.9527242879299826, consonant 0.9439162455132297, total: 0.8967152468621715, y (160672, 3)\n",
      "628it [00:31, 20.20it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9280846419523436, vowel 0.9799549285909254, consonant 0.9752437381022959, total: 0.9528419876494771, y (40168, 3)\n",
      "[14/30] train_loss: 0.25493 train_score: 0.89672 valid_loss: 0.11849 valid_score: 0.95284\n",
      "Validation loss decreased (0.122345 --> 0.118492).  Saving model ...\n",
      "628it [03:28,  3.01it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8530969372396738, vowel 0.9553465075200163, consonant 0.9430983701156969, total: 0.9011596880287652, y (160672, 3)\n",
      "628it [00:30, 20.52it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9279194507880814, vowel 0.9789711535754058, consonant 0.9712435271365771, total: 0.9515133955720365, y (40168, 3)\n",
      "[15/30] train_loss: 0.24428 train_score: 0.90116 valid_loss: 0.11644 valid_score: 0.95151\n",
      "Validation loss decreased (0.118492 --> 0.116443).  Saving model ...\n",
      "628it [03:28,  3.01it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8562154700748185, vowel 0.9558454644679312, consonant 0.9484600355705136, total: 0.9041841100470204, y (160672, 3)\n",
      "628it [00:31, 20.24it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9328078106047807, vowel 0.9796779399914926, consonant 0.9772150895946209, total: 0.9556271626989187, y (40168, 3)\n",
      "[16/30] train_loss: 0.23822 train_score: 0.90418 valid_loss: 0.11331 valid_score: 0.95563\n",
      "Validation loss decreased (0.116443 --> 0.113309).  Saving model ...\n",
      "628it [03:27,  3.03it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8633763131604126, vowel 0.958313485753396, consonant 0.9498284709106379, total: 0.9087236457462149, y (160672, 3)\n",
      "628it [00:31, 20.14it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9278288269311452, vowel 0.9790580326465542, consonant 0.9746510197229395, total: 0.952341676557946, y (40168, 3)\n",
      "[17/30] train_loss: 0.22446 train_score: 0.90872 valid_loss: 0.11138 valid_score: 0.95234\n",
      "Validation loss decreased (0.113309 --> 0.111385).  Saving model ...\n",
      "628it [03:28,  3.01it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8674982421859393, vowel 0.9594569074326965, consonant 0.9508881924172851, total: 0.9113353960554651, y (160672, 3)\n",
      "628it [00:31, 20.00it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9365818098289888, vowel 0.9810686151250626, consonant 0.9774921904674968, total: 0.9579311063126342, y (40168, 3)\n",
      "[18/30] train_loss: 0.21956 train_score: 0.91134 valid_loss: 0.10582 valid_score: 0.95793\n",
      "Validation loss decreased (0.111385 --> 0.105818).  Saving model ...\n",
      "628it [03:27,  3.03it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8731621999707704, vowel 0.9605548165002172, consonant 0.9535846261363187, total: 0.9151159606445192, y (160672, 3)\n",
      "628it [00:31, 20.19it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9395745829898569, vowel 0.9809839861247607, consonant 0.9775759395384159, total: 0.9594272729107225, y (40168, 3)\n",
      "[19/30] train_loss: 0.21024 train_score: 0.91512 valid_loss: 0.10531 valid_score: 0.95943\n",
      "Validation loss decreased (0.105818 --> 0.105312).  Saving model ...\n",
      "628it [03:27,  3.03it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8777863263485882, vowel 0.963069123816744, consonant 0.9536767873826834, total: 0.918079640974151, y (160672, 3)\n",
      "628it [00:31, 20.18it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9406451172132753, vowel 0.9831101514752194, consonant 0.9815031813039001, total: 0.9614758918014175, y (40168, 3)\n",
      "[20/30] train_loss: 0.20334 train_score: 0.91808 valid_loss: 0.10509 valid_score: 0.96148\n",
      "Validation loss decreased (0.105312 --> 0.105090).  Saving model ...\n",
      "628it [03:27,  3.03it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.881321345469782, vowel 0.9622970215842411, consonant 0.9565480445668687, total: 0.9203719392726685, y (160672, 3)\n",
      "628it [00:30, 20.29it/s]                                                        \n",
      "=================Validation=================\n",
      "recall: grapheme 0.9421388626270965, vowel 0.9835852442298276, consonant 0.9765397816869206, total: 0.9611006877927353, y (40168, 3)\n",
      "[21/30] train_loss: 0.19685 train_score: 0.92037 valid_loss: 0.10362 valid_score: 0.96110\n",
      "Validation loss decreased (0.105090 --> 0.103616).  Saving model ...\n",
      "628it [03:27,  3.02it/s]                                                        \n",
      "=================Train=================\n",
      "recall: grapheme 0.8846424967499855, vowel 0.9633449472486805, consonant 0.9581944859276315, total: 0.9227061066690707, y (160672, 3)\n",
      "  0%|                                                   | 0/627 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BengaliAI EfficientNet-B1 + GridMask + CV .ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
